{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiqfqdF0UHdiMD7b0AOpVr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m3CH11n9whJk"},"outputs":[],"source":["#@title Import relevant modules\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from matplotlib import pyplot as plt\n","\n","# The following lines adjust the granularity of reporting.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format\n","\n","# The following line improves formatting when ouputting NumPy arrays.\n","np.set_printoptions(linewidth = 200)"]},{"cell_type":"code","source":["(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"],"metadata":{"id":"fvo7OYfvwnZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train_normalized = x_train / 255.0\n","x_test_normalized = x_test / 255.0\n","print(x_train_normalized[2900][10]) # Output a normalized row"],"metadata":{"id":"tap1MR4PwxeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define the plotting function\n","def plot_curve(epochs, hist, list_of_metrics):\n","  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"\n","\n","  plt.figure()\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Value\")\n","\n","  for m in list_of_metrics:\n","    x = hist[m]\n","    plt.plot(epochs[1:], x[1:], label=m)\n","\n","  plt.legend()\n","\n","print(\"Loaded the plot_curve function.\")"],"metadata":{"id":"C3aK3SeHw2B7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(my_learning_rate):\n","  \"\"\"Create and compile a deep neural net.\"\"\"\n","\n","  # All models in this course are sequential.\n","  model = tf.keras.models.Sequential()\n","\n","  # The features are stored in a two-dimensional 28X28 array.\n","  # Flatten that two-dimensional array into a one-dimensional\n","  # 784-element array.\n","  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n","\n","  # Define the first hidden layer.\n","  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n","\n","  # Define a dropout regularization layer.\n","  model.add(tf.keras.layers.Dropout(rate=0.2))\n","\n","  # Define the output layer. The units parameter is set to 10 because\n","  # the model must choose among 10 possible output values (representing\n","  # the digits from 0 to 9, inclusive).\n","  #\n","  # Don't change this layer.\n","  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n","\n","  # Construct the layers into a model that TensorFlow can execute.\n","  # Notice that the loss function for multi-class classification\n","  # is different than the loss function for binary classification.\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n","                loss=\"sparse_categorical_crossentropy\",\n","                metrics=['accuracy'])\n","\n","  return model\n","\n","\n","def train_model(model, train_features, train_label, epochs,\n","                batch_size=None, validation_split=0.1):\n","  \"\"\"Train the model by feeding it data.\"\"\"\n","\n","  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n","                      epochs=epochs, shuffle=True,\n","                      validation_split=validation_split)\n","\n","  # To track the progression of training, gather a snapshot\n","  # of the model's metrics at each epoch.\n","  epochs = history.epoch\n","  hist = pd.DataFrame(history.history)\n","\n","  return epochs, hist"],"metadata":{"id":"1iwdOiYjw_59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The following variables are the hyperparameters.\n","learning_rate = 0.003\n","epochs = 50\n","batch_size = 4000\n","validation_split = 0.2\n","\n","# Establish the model's topography.\n","my_model = create_model(learning_rate)\n","\n","# Train the model on the normalized training set.\n","epochs, hist = train_model(my_model, x_train_normalized, y_train,\n","                           epochs, batch_size, validation_split)\n","\n","# Plot a graph of the metric vs. epochs.\n","list_of_metrics_to_plot = ['accuracy']\n","plot_curve(epochs, hist, list_of_metrics_to_plot)\n","\n","# Evaluate against the test set.\n","print(\"\\n Evaluate the new model against the test set:\")\n","my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)"],"metadata":{"id":"jMVqm4awxDE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# It would take much too long to experiment\n","# fully with topography and dropout regularization\n","# rate. In the real world, you would\n","# also experiment with learning rate, batch size,\n","# and number of epochs.  Since you only have a\n","# few minutes, searching for trends can be helpful.\n","# Here is what we discovered:\n","#   * Adding more nodes (at least until 256 nodes)\n","#     to the first hidden layer improved accuracy.\n","#   * Adding a second hidden layer generally\n","#     improved accuracy.\n","#   * When the model contains a lot of nodes,\n","#     the model overfits unless the dropout rate\n","#     is at least 0.5.\n","\n","# We reached 98% test accuracy with the\n","# following configuration:\n","#   * One hidden layer of 256 nodes; no second\n","#     hidden layer.\n","#   * dropout regularization rate of 0.4\n","\n","# We reached 98.2% test accuracy with the\n","# following configuration:\n","#   * First hidden layer of 256 nodes;\n","#     second hidden layer of 128 nodes.\n","#   * dropout regularization rate of 0.2"],"metadata":{"id":"ibodDA29xGhO"},"execution_count":null,"outputs":[]}]}